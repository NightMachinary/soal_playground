#+TITLE: implementations/RAPIDS

* @great @starful/2.5k @Nvidia [[https://github.com/rapidsai/cuml][GitHub - rapidsai/cuml: cuML - RAPIDS Machine Learning Library]]
** @breadcrumbs
*** [[https://stackoverflow.com/questions/55922162/recommended-cudf-dataframe-construction][python - Recommended cudf Dataframe Construction - Stack Overflow]]
#+begin_example python
import cudf
import numba as nb

# Convert a Numba DeviceNDArray to a cuDF DataFrame
src = nb.cuda.to_device([[1, 2], [3, 4]])
dst = cudf.DataFrame(src)

print(type(dst), "\n", dst)
#+end_example

** [[https://github.com/rapidsai/cuml/issues/4493][rapidsai/cuml#4493 {QST} How do I add the RAPIDS libraries as a dependency to a poetry project?]]

** [[https://docs.rapids.ai/api/cuml/stable/api.html#clustering][cuML API Reference — cuml 21.12.00 documentation]]
*** k-means
**** [[https://github.com/rapidsai/cuml/blob/branch-22.02/notebooks/kmeans_demo.ipynb][K-Means Demo]]

**** [[https://github.com/rapidsai/cuml/blob/branch-22.02/notebooks/kmeans_mnmg_demo.ipynb][K-Means Multi-Node Multi-GPU (MNMG) Demo]]

**** [[https://github.com/rapidsai/cuml/issues/1262][rapidsai/cuml#1262 {QST}100k features dataset clustering with Kmeans on GPU/multiple GPUs with cuml]]
#+begin_quote
Unfortunately, we don't yet have a k-means algorithm that can be trained out-of-core (splitting host w/ GPU memory) but we do have a distributed k-means implementation, which will allow you to scale the training of you your dataset over multiple GPUs and multi-nodes. We are working on smart caching so that your data can be brought into the GPU, as needed, for different processing tasks.
#+end_quote

#+begin_quote
Perhaps using a distance-preserving method like random projections or mapping your features to some number of principal components? cuML contains these algorithms as well as GPU-accelerated implementations of UMAP and TSNE, which are popular non-linear dimensionality reduction techniques. All of theses algorithms, except for the last two, also allow you to train a model on a random sampling of your dataset and then batch transform the remaining data.
#+end_quote

*** spectral clustering
**** @WIP [[https://github.com/rapidsai/cuml/issues/263][rapidsai/cuml#263 {FEA} Python API for Spectral Clustering / Embedding]]

** GPU-accelerated HDBSCAN in cuML
*** [[https://developer.nvidia.com/blog/gpu-accelerated-hierarchical-dbscan-with-rapids-cuml-lets-get-back-to-the-future/][GPU-Accelerated Hierarchical DBSCAN with RAPIDS cuML – Let’s Get Back To The Future | NVIDIA Developer Blog]]

#+ATTR_HTML: :width 472
[[file:sklearn.org_imgs/20220116_160021_X6MJtC.png]]

** GPU-accelerated UMAP and PCA

* out-of-core
** [[https://stackoverflow.com/questions/70776711/how-do-i-do-out-of-core-i-e-incremental-clustering-on-big-data-e-g-300gb][machine learning - How do I do out-of-core (i.e., incremental) clustering on big data (e.g., 300GB) with cuML (RAPIDS)? - Stack Overflow]]

** [[https://github.com/rapidsai/cuml/issues/4494][rapidsai/cuml#4494 {QST} generate big data (~300GB) with cuml.dask.datasets.make_blobs]]

** [[https://docs.rapids.ai/api/cuml/stable/api.html#incremental-pca][cuML API Reference — cuml 21.12.00 documentation]]

** @todo1 [[https://github.com/rapidsai/dask-cuda/issues/57][rapidsai/dask-cuda#57 Out of Memory Sort Fails even with Spill over]]

** [[https://github.com/rapidsai/dask-cuda/issues/836][rapidsai/dask-cuda#836 {QST} How do I convert a `dask` array to a `dask_cuda` array?]]

** [[https://stackoverflow.com/questions/60089009/cuml-functions-running-on-dask-and-dask-cudf-manipulation]]

** [[https://stackoverflow.com/questions/60563599/multigpu-kmeans-clustering-with-rapids-freezes]]

** [[https://stackoverflow.com/questions/67132703/can-i-split-physical-gpus-into-multiple-logical-virtual-gpus-and-pass-them-to-da]]
