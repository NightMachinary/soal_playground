#+TITLE: implementations/Spark

* [[https://spark.apache.org/docs/latest/ml-clustering.html][Clustering - Spark 3.2.0 Documentation]]

** streaming, out-of-core
*** [[https://spark.apache.org/docs/latest/streaming-programming-guide.html#initializing][Spark Streaming - Spark 3.2.0 Documentation]]

*** [[https://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means][Clustering - RDD-based API - Spark 3.2.0 Documentation]]

* distributed hyperparameter search using =sklearn= and =joblib-spark=
:PROPERTIES:
:SOURCE: https://stackoverflow.com/questions/38187637/integrating-scikit-learn-with-pyspark
:END:
#+begin_example python
from sklearn.utils import parallel_backend
from sklearn.model_selection import cross_val_score
from sklearn import datasets
from sklearn import svm
from joblibspark import register_spark

register_spark() # register spark backend

iris = datasets.load_iris()
clf = svm.SVC(kernel='linear', C=1)
with parallel_backend('spark', n_jobs=3):
  scores = cross_val_score(clf, iris.data, iris.target, cv=5)

print(scores)
#+end_example


* [[https://stackoverflow.com/questions/70886219/is-it-possible-to-use-a-dask-array-as-input-for-pyspark][apache spark - Is it possible to use a `dask` array as input for `pyspark`? - Stack Overflow]]

It seems this is not possible, and we should just load the data from the disk in the first place.

** [[https://stackoverflow.com/questions/66369505/convert-dask-dataframe-to-spark-dataframe-using-python][pandas - Convert Dask Dataframe to Spark dataframe using Python - Stack Overflow]]
