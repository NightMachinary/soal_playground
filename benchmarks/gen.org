#+TITLE: benchmarks/gen

* @unmaintained/2018 [[https://github.com/CODAIT/spark-bench][GitHub - CODAIT/spark-bench: Benchmark Suite for Apache Spark]]

* =dask-ml=
** [[https://dask-ml-benchmarks.readthedocs.io/en/latest/auto_examples/plot_spectral_clustering.html][Spectral Clustering Example ‚Äî Dask-ML Benchmarks 1.0.0 documentation]]

* scikit-learn-contrib/hdbscan
** @correctness [[https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/Comparing%20Clustering%20Algorithms.ipynb][Comparing Clustering Algorithms.ipynb]]

** [[https://colab.research.google.com/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/Benchmarking%20scalability%20of%20clustering%20implementations%202D%20v0.7.ipynb][Benchmarking Python Clustering Algorithms on 2D Data]]

** [[https://colab.research.google.com/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/Benchmarking%20scalability%20of%20clustering%20implementations-v0.7.ipynb][Benchmarking Performance and Scaling of Python Clustering Algorithms]]
#+ATTR_HTML: :width 316
[[file:gen.org_imgs/20220116_203308_hugZ1c.png]]
#+ATTR_HTML: :width 326
[[file:gen.org_imgs/20220116_203758_NFtRLH.png]]

#+begin_quote
If we're looking for scaling we can write off the scipy single linkage implementation -- if even we didn't hit the RAM limit the  ùëÇ(ùëõ2)  scaling is going to quickly catch up with us. Fastcluster has the same asymptotic scaling, but is heavily optimized to being the constant down much lower -- at this point it is still keeping close to the faster algorithms. It's asymtotics will still catch up with it eventually however.

In practice this is going to mean that for larger datasets you are going to be very constrained in what algorithms you can apply: if you get enough datapoints only K-Means, DBSCAN, and HDBSCAN will be left.
#+end_quote


** [[https://colab.research.google.com/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/Performance%20data%20generation%20.ipynb][Performance timings data generation]]
#+begin_quote
We need to generate data comparing performance of the reference implementation of HDBSCAN and various historical versions of the hdbscan library. We need to do this varying over dataset size so we can get an idea of scaling, and we also need to consider various dimension sizes. To get all this done we'll need some handy modules: sklearn.datasets to generate fake data for clustering ...
#+end_quote
