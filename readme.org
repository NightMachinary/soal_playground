#+TITLE: soal_playground

This file is authored in org-mode markup, and it is better viewed [[https://github.com/NightMachinary/soal_playground/raw/master/readme.org][raw]] than the default Github rendering view.

* project todos
** periphal
*** @upstreamBug Jupyter memory leak
**** [[https://colab.research.google.com/drive/1UpqpMbb6fpCZFDXNZ-Q5i72aAqn8R2cI?usp=sharing][reproduction steps]]

**** [[https://github.com/ipython/ipython/issues/3452#thread-subscription-status][ipython/ipython#3452 Memory leak even when cache_size = 0 and history_length = 0 or history_length = 1]]

*** @toread
**** Murphy, K. P. (2022). Probabilistic Machine Learning: An Introduction. MIT Press.
***** chapter 21 (clustering)

*** preprocessing
**** dimension reduction
#+begin_quote
In very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.
#+end_quote

***** UMAP
****** Is UMAP good for this purpose? I have heard it's more suited for visualizations, not analysis.

***** PCA

***** a feature selection technique like mutual information or gini impurity, to see if you can prioritize by feature importance and drop some of the less important ones
****** Does this make any sense?

** phase I
*** [[./data/datasets.org][Find good datasets.]]

*** benchmark a clustering algorithm (e.g., k-means) on:
**** scalability
***** feature size (10k needed)

**** time

**** memory

**** parallelism on CPUs

**** GPU/TPU support

**** How much can it saturate the computing device?

**** correctness
***** internal clustering metrics?

***** completeness score

***** homogeneity score

**** flexibility of the implementation
***** hyperparameters

*** Find other clustering algorithms and repeat.
